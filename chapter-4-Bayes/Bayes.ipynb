{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58f7643a-93ba-4ab0-8456-9a0d3c9c9f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "分类器有时会产生错误结果，这时可以要求分类\n",
    "器给出一个最优的类别猜测结果，同时给出这个猜测的概率估计值。\n",
    "概率论是许多机器学习算法的基础，所以深刻理解这一主题就显得十分重\n",
    "要。\n",
    "\"\"\"\n",
    "\n",
    "# 基于贝叶斯决策理论的分类方法\n",
    "\"\"\"\n",
    "朴素贝叶斯\n",
    "优点：在数据较少的情况下仍然有效，可以处理多类别问题。\n",
    "缺点：对于输入数据的准备方式较为敏感。\n",
    "适用数据类型：标称型数据。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "假设有位读者找到了描述图中两类数据的统计参数。（暂且不用管如何找\n",
    "到描述这类数据的统计参数，第10章会详细介绍。）我们现在用p1(x,y)\n",
    "表示数据点(x,y)属于类别1（图中用圆点表示的类别）的概率, 用p2(x,y)\n",
    "表示数据点(x,y)属于类别2（图中用三角形表示的类别）的概率，那么对于\n",
    "一个新数据点(x,y)，可以用下面的规则来判断它的类别：\n",
    "如果 p1(x,y) > p2(x,y)，那么类别为1。\n",
    "如果 p2(x,y) > p1(x,y)，那么类别为2。\n",
    "也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心\n",
    "思想，即选择具有最高概率的决策。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "使用p1( )和p2( )只是\n",
    "为了尽可能简化描述，而真正需要计算和比较的是p(c₁|x, y)和p(c₂|x,\n",
    "y)。这些符号所代表的具体意义是：给定某个由x、y表示的数据点，那么\n",
    "该数据点来自类别c₁的概率是多少？数据点来自类别c₂的概率又是多少 \n",
    "注意这些概率与刚才给出的概率p(x, y|c₁)并不一样，不过可以使用贝叶\n",
    "斯准则来交换概率中条件与结果。具体地，应用贝叶斯准则得到：\n",
    "p(c|x) = p(x|c)p(c) / p(x)\n",
    "使用这些定义，可以定义贝叶斯分类准则为：\n",
    "如果P(c₁|x, y) > P(c₂|x, y)，那么属于类别c₁。\n",
    "如果P(c₁|x, y) < P(c₂|x, y)，那么属于类别c₂。\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "4.4 使用朴素贝叶斯进行文档分类\n",
    "我们可以观察文档中出\n",
    "现的词，并把每个词的出现或者不出现作为一个特征，这样得到的特征数\n",
    "目就会跟词汇表中的词目一样多。\n",
    "\n",
    "\n",
    "朴素贝叶斯的一般过程：\n",
    "1. 收集数据：可以使用任何方法。本章使用RSS源。\n",
    "2. 准备数据：需要数值型或者布尔型数据\n",
    "3. 分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果\n",
    "更好。\n",
    "4. 训练算法：计算不同的独立特征的条件概率。\n",
    "5. 测试算法：计算错误率。\n",
    "6. 使用算法：一个常见的朴素贝叶斯应用是文档分类。可以在任意的分\n",
    "类场景中使用朴素贝叶斯分类器，不一定非要是文本。\n",
    "\n",
    "\n",
    "朴素贝叶斯分类器通常有两种实现方式：一种基于贝努利模型实现，一种基于多项式模型实现。\n",
    "这里采用前一种实现方式。该实现方式中并不考虑词在文档中出现的次数，只考虑出不出现，因此\n",
    "在这个意义上相当于假设词是等权重的。4.5.4节给出的实际上是多项式模型，它考虑词在文档中的\n",
    "出现次数。——译者注\n",
    "\n",
    "4.5 使用Python进行文本分类\n",
    "要从文本中获取特征，需要先拆分文本。具体如何做呢？这里的特征是来\n",
    "自文本的词条（token），一个词条是字符的任意组合。可以把词条想象为\n",
    "单词，也可以使用非单词词条，如URL、IP地址或者任意其他字符串。然\n",
    "后将每一个文本片段表示为一个词条向量，其中值为1表示词条出现在文档\n",
    "中，0表示词条未出现。\n",
    "\"\"\"\n",
    "\n",
    "def loadDataSet():\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                  ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                  ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                  ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how','to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']\n",
    "                  ]\n",
    "    classVec = [0,1,0,1,0,1]\n",
    "    return postingList, classVec\n",
    "    \n",
    "def createVocabList(dataSet):\n",
    "    \"\"\"\n",
    "    函数createVocabList()会创建一个包含在所有文档中出现的不重\n",
    "    复词的列表，为此使用了Python的set数据类型。将词条列表输给set构造\n",
    "    函数，set就会返回一个不重复词表。首先，创建一个空集合❶，然后将\n",
    "    每篇文档返回的新词集合添加到该集合中❷。操作符|用于求两个集合的\n",
    "    并集，这也是一个按位或（OR）操作符（参见附录C）。在数学符号表示\n",
    "    上，按位或操作与集合求并操作使用相同记号。\n",
    "    \"\"\"\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)\n",
    "\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbf40ecf-6855-410c-b46e-be478fba5983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is',\n",
       " 'dog',\n",
       " 'maybe',\n",
       " 'stop',\n",
       " 'posting',\n",
       " 'ate',\n",
       " 'garbage',\n",
       " 'has',\n",
       " 'mr',\n",
       " 'steak',\n",
       " 'to',\n",
       " 'problems',\n",
       " 'how',\n",
       " 'dalmation',\n",
       " 'so',\n",
       " 'help',\n",
       " 'stupid',\n",
       " 'licks',\n",
       " 'cute',\n",
       " 'please',\n",
       " 'buying',\n",
       " 'take',\n",
       " 'flea',\n",
       " 'I',\n",
       " 'food',\n",
       " 'quit',\n",
       " 'love',\n",
       " 'my',\n",
       " 'worthless',\n",
       " 'park',\n",
       " 'him',\n",
       " 'not']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOPosts,listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "myVocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5bc06ec-d11b-472d-bcf0-835bbc88a42d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2Vec(myVocabList, listOPosts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a960c090-0ac5-4c33-9963-bf5637641c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5.2 训练算法：从词向量计算概率\n",
    "\"\"\"\n",
    "我们重写贝叶斯准则，将之\n",
    "前的x、y 替换为w。粗体w表示这是一个向量，即它由多个数值组成。在\n",
    "这个例子中，数值个数与词汇表中的词个数相同。\n",
    "p(ci|w) = p(w|ci)p(ci)/p(w)\n",
    "首先可以通过类别i（侮辱性留言或非侮辱性留言）中\n",
    "文档数除以总的文档数来计算概率p(ci)。接下来计算p(w|ci)，这里就要\n",
    "用到朴素贝叶斯假设。如果将w展开为一个个独立特征，那么就可以将上\n",
    "述概率写作p(w0,w1,w2..wN|ci)。这里假设所有词都互相独立，该假设也\n",
    "称作条件独立性假设，它意味着可以使\n",
    "用p(w0|ci)p(w1|ci)p(w2|ci)...p(wN|ci)来计算上述概率，这就极大地\n",
    "简化了计算的过程。\n",
    "\"\"\"\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    \"\"\"\n",
    "    计算每个类别中的文档数目\n",
    "    对每篇训练文档：\n",
    "    对每个类别：\n",
    "    如果词条出现在文档中→ 增加该词条的计数值\n",
    "    增加所有词条的计数值\n",
    "    对每个类别：\n",
    "    对每个词条：\n",
    "    将该词条的数目除以总词条数目得到条件概率\n",
    "    返回每个类别的条件概率\n",
    "    \n",
    "    训练朴素贝叶斯分类器（二分类）\n",
    "    参数:\n",
    "        trainMatrix: 文档的词向量矩阵（每行是一个文档的词频向量）\n",
    "        trainCategory: 文档对应的类别标签（0或1）\n",
    "    返回:\n",
    "        p0Vect: 类别0下各单词的条件概率向量（未取对数）\n",
    "        p1Vect: 类别1下各单词的条件概率向量（未取对数）\n",
    "        pAbusive: 类别1（侮辱性文档）的先验概率\n",
    "    \"\"\"\n",
    "    numTrainDocs = len(trainMatrix)  # 训练文档总数\n",
    "    numWords = len(trainMatrix[0])   # 词汇表大小（特征数）\n",
    "\n",
    "    # 计算类别1（侮辱性文档）的先验概率\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)  # P(类别=1)\n",
    "\n",
    "    # 初始化概率统计变量\n",
    "    p0Num = zeros(numWords)  # 类别0下各单词的频数向量\n",
    "    p1Num = zeros(numWords)  # 类别1下各单词的频数向量\n",
    "    p0Denom = 0.0            # 类别0的总词频（用于归一化）\n",
    "    p1Denom = 0.0            # 类别1的总词频（用于归一化）\n",
    "\n",
    "    # 遍历每个训练文档\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            # 类别1的统计：累加词频向量和总词频\n",
    "            p1Num += trainMatrix[i]       # ❶ 向量相加，统计每个单词出现次数\n",
    "            p1Denom += sum(trainMatrix[i])  # 累加类别1的总词数\n",
    "        else:\n",
    "            # 类别0的统计：累加词频向量和总词频\n",
    "            p0Num += trainMatrix[i]       # 同上\n",
    "            p0Denom += sum(trainMatrix[i])  # 累加类别0的总词数\n",
    "    print(f\"类别1的总词数为{p1Denom}, 类别0的总词数为{p0Denom}\")\n",
    "    # 计算类别1和类别0的条件概率（未取对数）\n",
    "    p1Vect = p1Num / p1Denom  # ❷ 对每个单词频数归一化，得到P(单词|类别1)\n",
    "    p0Vect = p0Num / p0Denom  # 对每个单词频数归一化，得到P(单词|类别0)\n",
    "\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a3b09c4-6db9-49eb-8e1d-9f73234d31f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
       " ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
       " ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
       " ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
       " ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
       " ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOPosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68fb426c-49f9-4ff2-880b-199f5490d5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['is',\n",
       "  'dog',\n",
       "  'maybe',\n",
       "  'stop',\n",
       "  'posting',\n",
       "  'ate',\n",
       "  'garbage',\n",
       "  'has',\n",
       "  'mr',\n",
       "  'steak',\n",
       "  'to',\n",
       "  'problems',\n",
       "  'how',\n",
       "  'dalmation',\n",
       "  'so',\n",
       "  'help',\n",
       "  'stupid',\n",
       "  'licks',\n",
       "  'cute',\n",
       "  'please',\n",
       "  'buying',\n",
       "  'take',\n",
       "  'flea',\n",
       "  'I',\n",
       "  'food',\n",
       "  'quit',\n",
       "  'love',\n",
       "  'my',\n",
       "  'worthless',\n",
       "  'park',\n",
       "  'him',\n",
       "  'not'],\n",
       " 32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myVocabList, len(myVocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b75d23d6-c35d-4fec-95a7-ed7dd7a86b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "trainMat = []\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77e25046-e51e-4bb0-b9e0-9d421831561c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0],\n",
       "  [0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainMat, len(trainMat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4df24b61-b065-4fea-9a8c-9dff40299302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea9d3c87-db6d-4459-ad9c-53ece036e298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00b17cd9-4e35-433d-8b6e-77de0ccc90dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类别1的总词数为19.0, 类别0的总词数为24.0\n"
     ]
    }
   ],
   "source": [
    "p0V,p1V,pAb=trainNB0(trainMat,listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1b7b870-f255-4bb2-ac3a-c6533052d4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pAb # 数据集一半是好的，一半是坏的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c0e9e7d-51a1-49bc-8778-2f695e493043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04166667, 0.04166667, 0.        , 0.04166667, 0.        ,\n",
       "       0.04166667, 0.        , 0.04166667, 0.04166667, 0.04166667,\n",
       "       0.04166667, 0.04166667, 0.04166667, 0.04166667, 0.04166667,\n",
       "       0.04166667, 0.        , 0.04166667, 0.04166667, 0.04166667,\n",
       "       0.        , 0.        , 0.04166667, 0.04166667, 0.        ,\n",
       "       0.        , 0.04166667, 0.125     , 0.        , 0.        ,\n",
       "       0.08333333, 0.        ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "965c78dc-2e12-4df4-8504-39b16740a684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.10526316, 0.05263158, 0.05263158, 0.05263158,\n",
       "       0.        , 0.05263158, 0.        , 0.        , 0.        ,\n",
       "       0.05263158, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.15789474, 0.        , 0.        , 0.        ,\n",
       "       0.05263158, 0.05263158, 0.        , 0.        , 0.05263158,\n",
       "       0.05263158, 0.        , 0.        , 0.10526316, 0.05263158,\n",
       "       0.05263158, 0.05263158])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7df9b36f-c73c-4173-b008-c221b1b6d8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stupid'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myVocabList[16]  # stupid出现的次数 （3）/类别1总的词数19 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea597b27-eb9f-45e8-a7a4-d52468ccb339",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1VMat = trainMat[1] + trainMat[3] +  trainMat[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4e5dd3a-56d0-4613-a6f0-2f0494898e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(19)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p1VMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84841905-68e8-4c4f-90e1-ebd485a78281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5.3 测试算法：根据现实情况修改分类器\n",
    "\"\"\"\n",
    "利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档\n",
    "属于某个类别的概率，即计算p(w0|1)p(w1|1)p(w2|1)。如果其中一个概\n",
    "率值为0，那么最后的乘积也为0。为降低这种影响，可以将所有词的出现\n",
    "数初始化为1，并将分母初始化为2。\n",
    "\"\"\"\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)  # 训练文档总数\n",
    "    numWords = len(trainMatrix[0])   # 词汇表大小（特征数）\n",
    "\n",
    "    # 计算类别1（侮辱性文档）的先验概率\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)  # P(类别=1)\n",
    "\n",
    "    # 初始化概率统计变量\n",
    "    p0Num = ones(numWords)  # 类别0下各单词的频数向量\n",
    "    p1Num = ones(numWords)  # 类别1下各单词的频数向量\n",
    "    p0Denom = 2.0            # 类别0的总词频（用于归一化）\n",
    "    p1Denom = 2.0            # 类别1的总词频（用于归一化）\n",
    "\n",
    "    # 遍历每个训练文档\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            # 类别1的统计：累加词频向量和总词频\n",
    "            p1Num += trainMatrix[i]       # ❶ 向量相加，统计每个单词出现次数\n",
    "            p1Denom += sum(trainMatrix[i])  # 累加类别1的总词数\n",
    "        else:\n",
    "            # 类别0的统计：累加词频向量和总词频\n",
    "            p0Num += trainMatrix[i]       # 同上\n",
    "            p0Denom += sum(trainMatrix[i])  # 累加类别0的总词数\n",
    "    print(f\"类别1的总词数为{p1Denom}, 类别0的总词数为{p0Denom}\")\n",
    "    # 计算类别1和类别0的条件概率（未取对数）\n",
    "    \"\"\"\n",
    "    另一个遇到的问题是下溢出，这是由于太多很小的数相乘造成的。当计算\n",
    "    乘积p(w0|ci)p(w1|ci)p(w2|ci)...p(wN|ci)时，由于大部分因子都非常\n",
    "    小，所以程序会下溢出或者得到不正确的答案。（读者可以用Python尝试\n",
    "    相乘许多很小的数，最后四舍五入后会得到0。）一种解决办法是对乘积取\n",
    "    自然对数。在代数中有ln(a*b) = ln(a)+ln(b)，于是通过求对数可以\n",
    "    避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不\n",
    "    会有任何损失。\n",
    "    \n",
    "    函数f(x)与ln(f(x))会一块增大。这表明想求函数的最大值\n",
    "    时，可以使用该函数的自然对数来替换原函数进行求解\n",
    "    \"\"\"\n",
    "    p1Vect = log( p1Num / p1Denom)  # ❷ 对每个单词频数归一化，得到P(单词|类别1)\n",
    "    p0Vect = log(p0Num / p0Denom)  # 对每个单词频数归一化，得到P(单词|类别0)\n",
    "\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4a44ab4-b86d-4cfc-a263-b979e8a3c0e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainMat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m     thisDoc \u001b[38;5;241m=\u001b[39m array(setOfWords2Vec(myVocabList, testEntry))\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(testEntry,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassified as: \u001b[39m\u001b[38;5;124m'\u001b[39m,classifyNB(thisDoc,p0V,p1V,pAb))\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtestingNB\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m, in \u001b[0;36mtestingNB\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtestingNB\u001b[39m():\n\u001b[1;32m----> 9\u001b[0m     p0V,p1V,pAb \u001b[38;5;241m=\u001b[39m trainNB0(array(\u001b[43mtrainMat\u001b[49m),array(listClasses))\n\u001b[0;32m     10\u001b[0m     testEntry \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlove\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdalmation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m     thisDoc \u001b[38;5;241m=\u001b[39m array(setOfWords2Vec(myVocabList, testEntry))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainMat' is not defined"
     ]
    }
   ],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def testingNB():\n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print( testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "testingNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6449e82-42a8-4779-9355-b618a86d8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5.4 准备数据：文档词袋模型\n",
    "\n",
    "\"\"\"\n",
    "目前为止，我们将每个词的出现与否作为一个特征，这可以被描述为词集\n",
    "模型（set-of-words model）。如果一个词在文档中出现不止一次，这可能\n",
    "意味着包含该词是否出现在文档中所不能表达的某种信息，这种方法被称\n",
    "为词袋模型（bag-of-words model）。在词袋中，每个单词可以出现多次，\n",
    "而在词集中，每个词只能出现一次\n",
    "\"\"\"\n",
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2e68758-8677-4a04-9119-3ddc92d09456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>605</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2349</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3624</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4685</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 label                                               text  \\\n",
       "0         605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
       "1        2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
       "2        3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
       "3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
       "4        2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
       "\n",
       "   label_num  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          1  \n",
       "4          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"email.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c935912-2dce-47db-a34e-b7c917b0af06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5171, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "374a18a2-064b-4440-a160-fd4e84ddbdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cloud.tencent.com/pypi/simple\n",
      "Collecting scikit-learn\n",
      "  Downloading https://mirrors.cloud.tencent.com/pypi/packages/7f/9b/87961813c34adbca21a6b3f6b2bea344c43b30217a6d24cc437c6147f3e8/scikit_learn-1.7.2-cp310-cp310-win_amd64.whl (8.9 MB)\n",
      "     ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.8/8.9 MB 5.6 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 1.8/8.9 MB 4.6 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 2.9/8.9 MB 4.7 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 3.7/8.9 MB 4.7 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 5.0/8.9 MB 4.7 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 6.0/8.9 MB 4.8 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 6.6/8.9 MB 4.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 7.9/8.9 MB 4.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 8.9/8.9 MB 4.7 MB/s  0:00:01\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\shinelixie\\.conda\\envs\\machinelearninginaction\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading https://mirrors.cloud.tencent.com/pypi/packages/d1/84/55bc4881973d3f79b479a5a2e2df61c8c9a04fcb986a213ac9c02cfb659b/scipy-1.15.3-cp310-cp310-win_amd64.whl (41.3 MB)\n",
      "     ---------------------------------------- 0.0/41.3 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.5/41.3 MB 5.6 MB/s eta 0:00:08\n",
      "     - -------------------------------------- 1.8/41.3 MB 5.3 MB/s eta 0:00:08\n",
      "     --- ------------------------------------ 3.1/41.3 MB 5.4 MB/s eta 0:00:08\n",
      "     --- ------------------------------------ 3.9/41.3 MB 5.3 MB/s eta 0:00:07\n",
      "     ----- ---------------------------------- 5.2/41.3 MB 5.4 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 6.3/41.3 MB 5.3 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 7.6/41.3 MB 5.4 MB/s eta 0:00:07\n",
      "     -------- ------------------------------- 8.9/41.3 MB 5.4 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 10.0/41.3 MB 5.3 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 11.0/41.3 MB 5.3 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 11.8/41.3 MB 5.3 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 13.1/41.3 MB 5.2 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 14.2/41.3 MB 5.3 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 15.2/41.3 MB 5.3 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 16.3/41.3 MB 5.2 MB/s eta 0:00:05\n",
      "     ----------------- ---------------------- 17.6/41.3 MB 5.3 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 18.6/41.3 MB 5.3 MB/s eta 0:00:05\n",
      "     ------------------- -------------------- 19.7/41.3 MB 5.3 MB/s eta 0:00:05\n",
      "     -------------------- ------------------- 21.0/41.3 MB 5.3 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 22.0/41.3 MB 5.3 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 23.3/41.3 MB 5.3 MB/s eta 0:00:04\n",
      "     ----------------------- ---------------- 24.4/41.3 MB 5.3 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 25.4/41.3 MB 5.3 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 26.5/41.3 MB 5.3 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 27.5/41.3 MB 5.3 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 28.6/41.3 MB 5.3 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 29.6/41.3 MB 5.3 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 30.7/41.3 MB 5.3 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 31.7/41.3 MB 5.2 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 32.8/41.3 MB 5.2 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 33.8/41.3 MB 5.2 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 34.6/41.3 MB 5.2 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 35.9/41.3 MB 5.2 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 37.0/41.3 MB 5.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 37.7/41.3 MB 5.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 38.8/41.3 MB 5.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 39.8/41.3 MB 5.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  40.9/41.3 MB 5.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 41.3/41.3 MB 5.1 MB/s  0:00:08\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached https://mirrors.cloud.tencent.com/pypi/packages/1e/e8/685f47e0d754320684db4425a0967f7d3fa70126bffd76110b7009a0090f/joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached https://mirrors.cloud.tencent.com/pypi/packages/32/d5/f9a850d79b0851d1d4ef6456097579a9005b31fea68726a4ae5f2d82ddd9/threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ---------------------------------------- 4/4 [scikit-learn]\n",
      "\n",
      "Successfully installed joblib-1.5.2 scikit-learn-1.7.2 scipy-1.15.3 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fe3a339-0148-4592-bd30-f39c090a0600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 示例：使用朴素贝叶斯过滤垃圾邮件\n",
    "\n",
    "\"\"\"\n",
    "示例：使用朴素贝叶斯对电子邮件进行分类\n",
    "1. 收集数据：提供文本文件。\n",
    "2. 准备数据：将文本文件解析成词条向量。\n",
    "3. 分析数据：检查词条确保解析的正确性。\n",
    "4. 训练算法：使用我们之前建立的trainNB0()函数。\n",
    "5. 测试算法：使用classifyNB()，并且构建一个新的测试函数来计\n",
    "算文档集的错误率。\n",
    "6. 使用算法：构建一个完整的程序对一组文档进行分类，将错分的文\n",
    "档输出到屏幕上。\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "import re\n",
    "from numpy import array, log\n",
    "\n",
    "# 1. 文本解析函数（分词处理）\n",
    "def textParse(text):\n",
    "    tokens = re.split(r'\\W+', text)  # 按非字母字符分割\n",
    "    return [tok.lower() for tok in tokens if len(tok) > 2]  # 过滤短词并转小写\n",
    "\n",
    "# 2. 构建词汇表\n",
    "def createVocabList(docList):\n",
    "    vocabSet = set()\n",
    "    for doc in docList:\n",
    "        vocabSet.update(doc)  # 更高效的集合操作\n",
    "    return list(vocabSet)\n",
    "\n",
    "# 3. 词袋模型向量化\n",
    "\n",
    "# 4. 训练朴素贝叶斯（带拉普拉斯平滑）\n",
    "\n",
    "# 5. 分类函数\n",
    "\n",
    "# 6. 主测试函数\n",
    "def spamTest(csv_path='email.csv', sample_size=5000, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    改进版测试函数：确保训练集和测试集中spam/ham比例为5:5\n",
    "    参数:\n",
    "        csv_path: CSV文件路径（需包含text和label_num列）\n",
    "        sample_size: 总样本量（默认500）\n",
    "        test_ratio: 测试集比例（默认20%）\n",
    "    \"\"\"\n",
    "    # 读取CSV数据\n",
    "    # df = pd.read_csv(csv_path)\n",
    "\n",
    "    df['label_num'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "    # 确保数据平衡：筛选等量的spam和ham\n",
    "    spam_df = df[df['label_num'] == 1].iloc[:sample_size//2]  # 取一半spam\n",
    "    ham_df = df[df['label_num'] == 0].iloc[:sample_size//2]   # 取一半ham\n",
    "    balanced_df = pd.concat([spam_df, ham_df]).sample(frac=1, random_state=42)  # 合并后打乱\n",
    "    \n",
    "    # 数据预处理\n",
    "    docList = []\n",
    "    labelList = []\n",
    "    for _, row in balanced_df.iterrows():\n",
    "        docList.append(textParse(row['text']))\n",
    "        labelList.append(row['label_num'])\n",
    "    \n",
    "    # 构建词汇表\n",
    "    vocabList = createVocabList(docList)\n",
    "    \n",
    "    # 分层划分训练/测试集（保持5:5比例）\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_indices, test_indices = train_test_split(\n",
    "        range(len(labelList)), \n",
    "        test_size=test_ratio, \n",
    "        stratify=labelList,  # 关键：按标签分层抽样\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 训练数据准备\n",
    "    trainMatrix = [bagOfWords2VecMN(vocabList, docList[i]) for i in train_indices]\n",
    "    trainLabels = [labelList[i] for i in train_indices]\n",
    "    \n",
    "    # 训练模型\n",
    "    p0V, p1V, pSpam = trainNB0(array(trainMatrix), array(trainLabels))\n",
    "    \n",
    "    # 测试评估\n",
    "    errorCount = 0\n",
    "    for i in test_indices:\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[i])\n",
    "        if classifyNB(array(wordVector), p0V, p1V, pSpam) != labelList[i]:\n",
    "            errorCount += 1\n",
    "    \n",
    "    # 打印分类报告\n",
    "    print(\"\\n===== 平衡数据集分类结果 =====\")\n",
    "    print(f\"总邮件数: {sample_size} (spam={sample_size//2}, ham={sample_size//2})\")\n",
    "    print(f\"测试集大小: {len(test_indices)} (spam={sum(labelList[i] for i in test_indices)}, ham={len(test_indices)-sum(labelList[i] for i in test_indices)})\")\n",
    "    print(f\"分类错误数: {errorCount}\")\n",
    "    print(f\"错误率: {errorCount/len(test_indices):.2%}\")\n",
    "    \n",
    "    return errorCount / len(test_indices)\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "898a3386-6ecb-46be-b0c2-f1c1b991e97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类别1的总词数为178044.0, 类别0的总词数为234519.0\n",
      "\n",
      "===== 平衡数据集分类结果 =====\n",
      "总邮件数: 5000 (spam=2500, ham=2500)\n",
      "测试集大小: 800 (spam=300, ham=500)\n",
      "分类错误数: 16\n",
      "错误率: 2.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shinelixie\\AppData\\Local\\Temp\\ipykernel_7804\\987284888.py:91: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  print(f\"测试集大小: {len(test_indices)} (spam={sum(labelList[i] for i in test_indices)}, ham={len(test_indices)-sum(labelList[i] for i in test_indices)})\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.02"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b56fc63d-ab7c-446b-9dd4-acabb5bb996a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '”' (U+201D) (2589161028.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[19], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    ”\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '”' (U+201D)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d9d139-92e8-4064-ad3a-6da3c01d5d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
